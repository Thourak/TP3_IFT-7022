{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65179fa",
   "metadata": {},
   "source": [
    "# *Notebook* à utiliser pour faire le travail pratique # 3 sur l'analyse d'incidents.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargements Modèles et Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2233dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cpu\n"
     ]
    }
   ],
   "source": [
    "# Charger les modèle / Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n",
    "# Vérification de la disponibilité du GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bfabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "file_path = 'data/dev_examples.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67ca057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('analyze:  At around 10:00 p.m. on November 10  2013  Employee #1  with Villager  Construction Inc.  with a coworker  were using an asphalt milling machine  (Wirtgen; Model Number: W2100) to grind out existing asphalt from an  interstate at a railroad bridge overpass. Employee # 1 was standing on the  ground  checking the depth of the cut into the asphalt  using a handheld  pendant attached to the machine. The pedant could stretch out from ten to 15  ft. This allowed Employee #1 to walk back and forth  checking the cut. The  operator was on the top of the milling machine  controlling the operation of  the machine and ensuring that the milling machine and dump truck (driven by a  second coworker  who worked for an independent trucking service) kept a safe  working distance. A different company  Protective Services Inc. (PSI)  was  responsible for the traffic control of the job site and had shut down the  inside lane of a three lane section of the interstate  so that work could be  conducted on that lane. The entire work zone was approximately two miles long   from start to finish. Employee #1 and the operator of the milling machine had  completed milling four sections (eight total passes) of the inside lane at the  bridge overpasses and were waiting for PSI to shut down the center lane. Dual  lane shut down of the inside and center lanes of the interstate was completed  around 9:30 p.m.  and Employee #1 and the milling machine operator milled two  sections (four total passes) of the center lane. Once both sides of the  overpass were milled out  approximately 200 ft on each side  Employee #1 and  the operator of the milling machine moved the milling machine down the  interstate  approximately1 000 ft  to a railroad overpass and began setting up  to mill the center lane sections. The truck driver backed his truck into  position and remained in the truck to move the truck slowly forward as milling  took place. Employee #1 was positioned between the milling machine and the  concrete median dividers  inside the coned off work zone. The lanes of travel  were approximately 12 ft wide  so the milling machine made two passes  since  it can only cut seven ft wide on each section to cover the entire lane.  Employee #1 was standing approximately three ft in the far inside lane on the  ground between milling machine and interior median wall inside of the approved  traffic control set up  and approximately midway up the machine and 17 ft from  the traffic control devices and flow of traffic. The milling machine was  approximately nine ft wide by 50 ft long  while operating. Employee #1 was  guarded by the machine from the flow of traffic. Approximately five to ten  minutes into the first pass  the milling machine operator noticed lights  hitting the reflectors on the inside wall and turned briefly to see a vehicle  coming. The operator thought it was the Project manager coming to check on the  status of the project. Then  the operator realized that the oncoming vehicle  was not equipped with a strobe  as required in work zones. The operator turned  and yelled for Employee # 1 to run for safety  as a Chevrolet Tahoe came down  the inside lane where Employee #1 was standing. The driver of the Tahoe  continued traveling in the far inside lane of the work zone  where Employee #1  was struck and thrown some 100 ft from where he was originally standing. The  vehicle was moving approximately 45 mph per hour. As he was transferred to a  hospital by emergency personnel  Employee #1 was treated for severe trauma   lacerations  fractures  and contusions to the body and head. Employee #1 was  pronounced dead at the hospital. The driver of the vehicle disregarded the  traffic control set up  all warning lights on the rear of the milling machine   and cone spacing of 100 ft. The construction work zone was set up correctly  with all signage  cone spacing  tapering  attenuators  and lighting; all of  the traffic control set up was approved by MUTCD for this type of tr  <extra_id_0> EVENT', \"['Employee #1  was struck and thrown'] <extra_id_1>\")\n"
     ]
    }
   ],
   "source": [
    "def create_input_data(data):\n",
    "    formatted_data = []\n",
    "\n",
    "    for item in data:\n",
    "        text = item['text']\n",
    "        arguments = item['arguments']\n",
    "        \n",
    "        for key, values in arguments.items():\n",
    "            input_text = f\"analyze: {text} <extra_id_0> {key}\"\n",
    "            target_text = f\"{values} <extra_id_1>\"\n",
    "            formatted_data.append((input_text, target_text))\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "# Création des question à donner au modèle\n",
    "dataset = create_input_data(data)\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction d'évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcule du score d'une modèle\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en comment et estime précision, rappel et F1.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if len(ground_truth_tokens) == 0 or len(prediction_tokens) == 0:\n",
    "        return int(ground_truth_tokens == prediction_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth): \n",
    "    \"\"\"Vérifie si les 2 textes sont quasi-identiques.\"\"\"\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    \"\"\"La fonction princiaple. Important de noter que ground_truths est une liste \n",
    "       parce qu'il peut y avoir plusieurs réponses possibles.\"\"\"\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740e5bc",
   "metadata": {},
   "source": [
    "# Modèle de maskage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb24f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour compléter un texte avec un masque\n",
    "def fill_in_the_blank(text):\n",
    "    input_text = f\"fill in the blank: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64348f5e",
   "metadata": {},
   "source": [
    "# Modèle Question-réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour répondre à une question\n",
    "def answer_question(question, context):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Génération des réponses\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "question = \"A human is smaller or taller than a cat?\"\n",
    "context = \"This revue notice that a cat mesure 50cm in average and a human 1.70m.\"\n",
    "output = answer_question(question, context)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, eval_fn):\n",
    "    for item in dataset:\n",
    "        input_text = item[0]\n",
    "        target_text = item[1]\n",
    "\n",
    "        output = generate_answer(input_text)\n",
    "        print(f\"Predicted: {output}, Target: {target_text}\")\n",
    "        # Apply evaluation function here if needed\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dataset_qa, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fdd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19c165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
