{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65179fa",
   "metadata": {},
   "source": [
    "# *Notebook* à utiliser pour faire le travail pratique # 3 sur l'analyse d'incidents.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques \n",
    "import torch\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifiez si un GPU est disponible et utilisez-le, sinon utilisez le CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargements des données et questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dev_examples.json', 'r') as file:\n",
    "  data = json.load(file)\n",
    "\n",
    "liste_questions =[\n",
    "  [\n",
    "    \"what happened ?\",\n",
    "    \"describe the event that occured ?\"\n",
    "  ],\n",
    "  [\n",
    "    \n",
    "    \"What was the employee doing right before the incident, and what led to it?\",\n",
    "    \"what was the main activity?\",\n",
    "    \"what was the activity?\"\n",
    "    \n",
    "\n",
    "  ],\n",
    "  [\n",
    "    \"Who is the main person involved in this incident?\",\n",
    "    \"Can you identify the individual involved in this incident?\"\n",
    "  ],\n",
    "  [\n",
    "     \"Where did this incident take place?\",\n",
    "     \"Can you specify the exact location where the incident occurred?\"\n",
    "  ],\n",
    "  [\n",
    "    \"When did this incident occur?\",\n",
    "    \"Can you provide the precise date and time of the incident?\"\n",
    "\n",
    "  ],\n",
    "  [\n",
    "    \"What is the  cause ?\",\n",
    "    \"what is the cause of incident?\"\n",
    "    \"describe the cause of incident\"\n",
    "    \"Explain what caused this incident.\"\n",
    "  ],\n",
    "  [\n",
    "     \"What equipment was in this incident?\",\n",
    "     \"Can you name the tools or equipment used at the time of the incident?\"\n",
    "  ],\n",
    "  [\n",
    "    \n",
    "    \"what kind of injury was there?\",\n",
    "    \"Describe the physical damages  from the incident.\"\n",
    "    \n",
    "  ],\n",
    "  [\n",
    "    \"Who was injured in this incident?\",\n",
    "    \"Can you identify the person who suffered injuries?\"\n",
    "  ],\n",
    "  [\n",
    "    \"Which body parts were affected by the injuries?\",\n",
    "    \"Describe the part of the body affected by the incident.\"\n",
    "  ],\n",
    "  [\n",
    "    \n",
    "    \"who was dead?\",\n",
    "    \"wich person died?\"\n",
    "    \n",
    "  ],\n",
    "  [\n",
    "    \"Were there any substances involved in this incident?\",\n",
    "    \"Name the substances that played a role in this incident.\"\n",
    "  ]\n",
    "  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b3cfb",
   "metadata": {},
   "source": [
    "## Configuration du modèle de question-réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070ded79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "#Fonction pour exécuter le modèle de question-réponse\n",
    "def question_model(question,context) : \n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': context\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    return res['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78abe16",
   "metadata": {},
   "source": [
    "## les fonctions d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en comment et estime précision, rappel et F1.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if len(ground_truth_tokens) == 0 or len(prediction_tokens) == 0:\n",
    "        return int(ground_truth_tokens == prediction_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth): \n",
    "    \"\"\"Vérifie si les 2 textes sont quasi-identiques.\"\"\"\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    \"\"\"La fonction princiaple. Important de noter que ground_truths est une liste \n",
    "       parce qu'il peut y avoir plusieurs réponses possibles.\"\"\"\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109cf1d",
   "metadata": {},
   "source": [
    "## Analyse d'incidents et des questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste des clés d'arguments pour les incidents\n",
    "arguments_keys = ['EVENT', 'ACTIVITY', 'WHO', 'WHERE', 'WHEN', 'CAUSE', 'EQUIPMENT', 'INJURY', 'INJURED', 'BODY-PARTS', 'DEATH', 'SUBSTANCE']\n",
    "#boucle sur les données et les questions\n",
    "for d in data[:10]:\n",
    "    context = d['text']\n",
    "    arguments = d['arguments']\n",
    "    print(\"context : \", context)\n",
    "    for key, questions in zip(arguments_keys, liste_questions):\n",
    "        if key in arguments:\n",
    "            print(\"Arguments : \", arguments[key])\n",
    "            for i, question in enumerate(questions):\n",
    "                res = question_model(question, context)\n",
    "\n",
    "                exact_match = metric_max_over_ground_truths(exact_match_score, res, arguments[key])\n",
    "                f1_value = metric_max_over_ground_truths(f1_score, res, arguments[key])\n",
    "                \n",
    "                print(f\"Q{i+1} : {question}\")\n",
    "                print(f\"R{i+1} : {res}\")\n",
    "                print('Exact match:', exact_match, '\\nF1:', f1_value)\n",
    "                print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9bbb08",
   "metadata": {},
   "source": [
    "## Choix du modèle et pertinence de l'approche\n",
    "\n",
    "Nous avons opté pour le modèle Roberta, plus précisément deepset/roberta-base-squad2, en raison de plusieurs avantages significatifs qui le rendent particulièrement adapté à la tâche de Question-Réponse (QA) dans le cadre de l'analyse d'incidents.\n",
    "\n",
    "Architecture pré-entraînée performante : Les modèles Roberta sont basés sur une architecture de transformateur pré-entraînée, qui a démontré une forte capacité à capturer des représentations sémantiques complexes. \n",
    "Cela se traduit par une meilleure compréhension du contexte et une réponse plus précise aux questions posées.\n",
    "\n",
    "En utilisant un modèle pré-entraîné comme deepset/roberta-base-squad2, nous pouvons profiter d'une base solide tout en ayant la flexibilité d'ajuster le modèle pour des domaines spécifiques, comme notre analyse d'incidents. Cela permet une personnalisation efficace pour des contextes particuliers.\n",
    "\n",
    "le choix de Roberta repose sur son succès antérieur dans le domaine du QA, sa robustesse face à des contextes variés, et son potentiel d'adaptation aux exigences spécifiques de l'analyse d'incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d734254",
   "metadata": {},
   "source": [
    "## Évaluation de l'Impact de Différentes Formulations de Questions \n",
    "Formulations Réussies:\n",
    "les questions ouvertes telles que \"what happened?\" ont généré des réponses précises, offrant une vue d'ensemble de l'événement.\n",
    "La question \"Where did this incident take place?\" a conduit à une réponse précise, identifiant correctement le lieu comme \"railroad bridge overpass.\"\n",
    "\n",
    "Formulations Problématiques:\n",
    "\n",
    "Les questions complexes comme \"What is the cause? Describe the cause of the incident. Explain what caused this incident.\" ont parfois conduit à des réponses moins claires.\n",
    "\n",
    "Influence de la Complexité de la Question:\n",
    "\n",
    "Les formulations simples du type \"Who?\" ou \"When?\" ont bien fonctionné, tandis que des questions complexes ont parfois abouti à des réponses moins cohérentes.\n",
    "\n",
    "Impact sur la Cohérence des Réponses:\n",
    "\n",
    "Des variations dans les formulations ont influencé la cohérence. Par exemple, \"What happened?\" et \"Describe the event that occurred?\" ont obtenu des réponses similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fdd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19c165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
