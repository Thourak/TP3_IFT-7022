{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65179fa",
   "metadata": {},
   "source": [
    "# *Notebook* à utiliser pour faire le travail pratique # 3 sur l'analyse d'incidents.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertForMaskedLM, pipeline\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargements Modèles et Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2233dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "# Charger les modèle / Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model_mask = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_mask, tokenizer=tokenizer)\n",
    "model_answer = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Vérification de la disponibilité du GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5233392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is a description of an incident :  At around 10:00 p.m. on November 10  2013  Employee #1  with Villager  Construction Inc.  with a coworker  were using an asphalt milling machine  (Wirtgen; Model Number: W2100) to grind out existing asphalt from an  interstate at a railroad bridge overpass. Employee # 1 was standing on the  ground  checking the depth of the cut into the asphalt  using a handheld  pendant attached to the machine. The pedant could stretch out from ten to 15  ft. This allowed Employee #1 to walk back and forth  checking the cut. The  operator was on the top of the milling machine  controlling the operation of  the machine and ensuring that the milling machine and dump truck (driven by a  second coworker  who worked for an independent trucking service) kept a safe  working distance. A different company  Protective Services Inc. (PSI)  was  responsible for the traffic control of the job site and had shut down the  inside lane of a three lane section of the interstate  so that work could be  conducted on that lane. The entire work zone was approximately two miles long   from start to finish. Employee #1 and the operator of the milling machine had  completed milling four sections (eight total passes) of the inside lane at the  bridge overpasses and were waiting for PSI to shut down the center lane. Dual  lane shut down of the inside and center lanes of the interstate was completed  around 9:30 p.m.  and Employee #1 and the milling machine operator milled two  sections (four total passes) of the center lane. Once both sides of the  overpass were milled out  approximately 200 ft on each side  Employee #1 and  the operator of the milling machine moved the milling machine down the  interstate  approximately1 000 ft  to a railroad overpass and began setting up  to mill the center lane sections. The truck driver backed his truck into  position and remained in the truck to move the truck slowly forward as milling  took place. Employee #1 was positioned between the milling machine and the  concre. The event of the incident is [MASK].', ['Employee #1  was struck and thrown'])\n",
      "{'contexte': ' At around 10:00 p.m. on November 10  2013  Employee #1  with Villager  Construction Inc.  with a coworker  were using an asphalt milling machine  (Wirtgen; Model Number: W2100) to grind out existing asphalt from an  interstate at a railroad bridge overpass. Employee # 1 was standing on the  ground  checking the depth of the cut into the asphalt  using a handheld  pendant attached to the machine. The pedant could stretch out from ten to 15  ft. This allowed Employee #1 to walk back and forth  checking the cut. The  operator was on the top of the milling machine  controlling the operation of  the machine and ensuring that the milling machine and dump truck (driven by a  second coworker  who worked for an independent trucking service) kept a safe  working distance. A different company  Protective Services Inc. (PSI)  was  responsible for the traffic control of the job site and had shut down the  inside lane of a three lane section of the interstate  so that work could be  conducted on that lane. The entire work zone was approximately two miles long   from start to finish. Employee #1 and the operator of the milling machine had  completed milling four sections (eight total passes) of the inside lane at the  bridge overpasses and were waiting for PSI to shut down the center lane. Dual  lane shut down of the inside and center lanes of the interstate was completed  around 9:30 p.m.  and Employee #1 and the milling machine operator milled two  sections (four total passes) of the center lane. Once both sides of the  overpass were milled out  approximately 200 ft on each side  Employee #1 and  the operator of the milling machine moved the milling machine down the  interstate  approximately1 000 ft  to a railroad overpass and began setting up  to mill the center lane sections. The truck driver backed his truck into  position and remained in the truck to move the truck slowly forward as milling  took place. Employee #1 was positioned between the milling machine and the  concre', 'question': 'What is the EVENT in the incident?', 'target': ['Employee #1  was struck and thrown']}\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "file_path = 'data/dev_examples.json'\n",
    "\n",
    "max_description_length = 2000\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def create_input_data_mask(data):\n",
    "    formatted_data = []\n",
    "\n",
    "    for item in data:\n",
    "        text = item['text']\n",
    "        arguments = item['arguments']\n",
    "\n",
    "        input_text_EVENT = f\"This is a description of an incident : {text[0:max_description_length]}. The event of the incident is [MASK].\"\n",
    "        input_text_ACTIVITY = f\"This is a description of an incident : {text[0:max_description_length]}. The activity of the incident is [MASK].\"\n",
    "        input_text_WHO = f\"This is a description of an incident : {text[0:max_description_length]}. The person concern by the incident is [MASK].\"\n",
    "        input_text_WHERE = f\"This is a description of an incident : {text[0:max_description_length]}. The location of the incident is [MASK].\"\n",
    "        input_text_WHEN = f\"This is a description of an incident : {text[0:max_description_length]}. The incident occur [MASK].\"\n",
    "        input_text_CAUSE = f\"This is a description of an incident : {text[0:max_description_length]}. The incident cause is [MASK].\"\n",
    "        input_text_EQUIPMENT = f\"This is a description of an incident : {text[0:max_description_length]}. The equipement use is [MASK].\"\n",
    "        input_text_INJURY = f\"This is a description of an incident : {text[0:max_description_length]}. The incident injury is [MASK].\"\n",
    "        input_text_INJURED = f\"This is a description of an incident : {text[0:max_description_length]}. The person injured is [MASK].\"\n",
    "        input_text_BODYPARTS = f\"This is a description of an incident : {text[0:max_description_length]}. The body part injured is [MASK].\"\n",
    "        input_text_DEATH = f\"This is a description of an incident : {text[0:max_description_length]}. The person who died is [MASK].\"\n",
    "\n",
    "        formatted_data.append((input_text_EVENT, arguments['EVENT']))\n",
    "        formatted_data.append((input_text_ACTIVITY, arguments['ACTIVITY']))\n",
    "        formatted_data.append((input_text_WHO, arguments['WHO']))\n",
    "        formatted_data.append((input_text_WHERE, arguments['WHERE']))\n",
    "        formatted_data.append((input_text_WHEN, arguments['WHEN']))\n",
    "        formatted_data.append((input_text_CAUSE, arguments['CAUSE']))\n",
    "        formatted_data.append((input_text_EQUIPMENT, arguments['EQUIPMENT']))\n",
    "        formatted_data.append((input_text_INJURY, arguments['INJURY']))\n",
    "        formatted_data.append((input_text_INJURED, arguments['INJURED']))\n",
    "        formatted_data.append((input_text_BODYPARTS, arguments['BODY-PARTS']))\n",
    "        formatted_data.append((input_text_DEATH, arguments['DEATH']))\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "def create_input_data_answer(data):\n",
    "    formatted_data = []\n",
    "\n",
    "    for item in data:\n",
    "        text = item['text'][0:max_description_length]\n",
    "        arguments = item['arguments']\n",
    "        \n",
    "        for key, values in arguments.items():\n",
    "            question = f\"What is the {key} in the incident?\"\n",
    "            target_text = values\n",
    "\n",
    "            item = {\n",
    "                'contexte': text,\n",
    "                'question': question,\n",
    "                'target': target_text\n",
    "            }\n",
    "            formatted_data.append(item)\n",
    "                \n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Création des question à donner au modèle\n",
    "dataset_mask = create_input_data_mask(data)\n",
    "dataset_answer = create_input_data_answer(data)\n",
    "\n",
    "with open('data/new_examples.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "custom_dataset_mask = create_input_data_mask(data)\n",
    "custom_dataset_answer = create_input_data_answer(data)\n",
    "\n",
    "print(dataset_mask[0])\n",
    "print(dataset_answer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction d'évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a80c8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcule du score d'une modèle\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Mettre en minuscule et retirer la ponctuation, des déterminants and les espaces.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Normalise les 2 textes, trouve ce qu'il y a en comment et estime précision, rappel et F1.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if len(ground_truth_tokens) == 0 or len(prediction_tokens) == 0:\n",
    "        return int(ground_truth_tokens == prediction_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth): \n",
    "    \"\"\"Vérifie si les 2 textes sont quasi-identiques.\"\"\"\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    \"\"\"La fonction princiaple. Important de noter que ground_truths est une liste \n",
    "       parce qu'il peut y avoir plusieurs réponses possibles.\"\"\"\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740e5bc",
   "metadata": {},
   "source": [
    "# Modèle de maskage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceb24f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score of the mask model in the dataset is : 0.18045454545454545.\n",
      "The average score of the mask model in the dataset is : 0.18.\n",
      "The average score of the mask model in the dataset is : 0.045454545454545456.\n",
      "The average score of the mask model in the dataset is : 0.045454545454545456.\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the mask model\n",
    "\n",
    "def evaluate_fill_model(dataset, metric_fn):\n",
    "    total_score = 0.0\n",
    "    for item in dataset:\n",
    "        input = item[0]\n",
    "        target = item[1]\n",
    "        output = fill_mask(input)\n",
    "        pred = output[0]['token_str']\n",
    "        pred = '' if pred == \"unknown\" else pred\n",
    "        total_score += metric_max_over_ground_truths(metric_fn, pred, target)\n",
    "\n",
    "    total_score /= len(dataset)\n",
    "\n",
    "    print(f\"The average score of the mask model in the dataset is : {total_score}.\")\n",
    "\n",
    "evaluate_fill_model(dataset_mask, f1_score)\n",
    "evaluate_fill_model(dataset_mask, exact_match_score)\n",
    "evaluate_fill_model(custom_dataset_mask, f1_score)\n",
    "evaluate_fill_model(custom_dataset_mask, exact_match_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64348f5e",
   "metadata": {},
   "source": [
    "# Modèle Question-réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5982566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, topN=20):\n",
    "    def get_top_answers(possible_starts, possible_ends, input_ids=20):\n",
    "        answers = []\n",
    "        for start, end in zip(possible_starts, possible_ends):\n",
    "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end+1]))\n",
    "            answers.append(answer)\n",
    "        return answers \n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")    \n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    model_out = model_answer(**inputs)\n",
    "     \n",
    "    answer_start_scores = model_out[\"start_logits\"]\n",
    "    answer_end_scores = model_out[\"end_logits\"]\n",
    "\n",
    "    possible_starts = np.argsort(answer_start_scores.cpu().detach().numpy()).flatten()[::-1][:topN]\n",
    "    possible_ends = np.argsort(answer_end_scores.cpu().detach().numpy()).flatten()[::-1][:topN]\n",
    "    \n",
    "    #get best answer\n",
    "    answer_start = torch.argmax(answer_start_scores)  \n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  \n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    answers = get_top_answers(possible_starts, possible_ends, input_ids )\n",
    "\n",
    "    return { \"answer\":answer,\"answer_start\":answer_start,\"answer_end\":answer_end,\"input_ids\":input_ids,\n",
    "            \"answer_start_scores\":answer_start_scores,\"answer_end_scores\":answer_end_scores,\"inputs\":inputs,\"answers\":answers,\n",
    "            \"possible_starts\":possible_starts,\"possible_ends\":possible_ends}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae20e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score of the question-response model in the dev dataset is : 0.12085948258850022.\n",
      "The average score of the question-response model in the dev dataset is : 0.00909090909090909.\n",
      "The average score of the question-response model in the dev dataset is : 0.12085948258850022.\n",
      "The average score of the question-response model in the dev dataset is : 0.00909090909090909.\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the answer model\n",
    "\n",
    "def evaluate_answer_model(dataset, metric_fn):\n",
    "    total_score = 0.0\n",
    "    for item in dataset:\n",
    "        output = answer_question(item['contexte'], item['question'])\n",
    "        total_score += metric_max_over_ground_truths(metric_fn, output['answer'], item['target'])\n",
    "\n",
    "    total_score /= len(dataset)\n",
    "    print(f\"The average score of the question-response model in the dev dataset is : {total_score}.\")\n",
    "\n",
    "evaluate_answer_model(dataset_answer, f1_score)\n",
    "evaluate_answer_model(custom_dataset_answer, exact_match_score)\n",
    "evaluate_answer_model(dataset_answer, f1_score)\n",
    "evaluate_answer_model(custom_dataset_answer, exact_match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa5340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fdd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19c165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
